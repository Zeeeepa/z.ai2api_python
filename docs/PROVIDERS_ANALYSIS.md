# Multi-Provider AI API Server - Comprehensive Provider Analysis

This document provides a detailed analysis of all AI providers integrated into the z.ai2api_python server, including their models, capabilities, authentication mechanisms, and tool support.

## üìä Provider Overview

| Provider | Models | Streaming | Tool Calling | Search | Thinking Mode | Auth Type |
|----------|--------|-----------|--------------|--------|---------------|-----------|
| **Z.AI** | 7 | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | Token Pool / Anonymous |
| **K2Think** | 1 | ‚úÖ | ‚ùå | ‚ùå | ‚úÖ | Cookie Session (Guest) |
| **LongCat** | 3 | ‚úÖ | ‚ùå | ‚úÖ | ‚ùå | Passport Token |
| **Qwen** | - | ‚ùå | ‚ùå | ‚ùå | ‚ùå | Not Implemented |

---

## ü§ñ Z.AI Provider

**File**: `app/providers/zai_provider.py`  
**Base URL**: `https://chat.z.ai`  
**Status**: ‚úÖ Fully Implemented

### Supported Models

| Model Name | Upstream ID | Description | Key Features |
|------------|-------------|-------------|--------------|
| `GLM-4.5` | `0727-360B-API` | Standard chat model | General purpose, balanced performance |
| `GLM-4.5-Thinking` | `0727-360B-API` | Reasoning model | Shows reasoning process, transparent thinking |
| `GLM-4.5-Search` | `0727-360B-API` | Search-enabled model | Real-time web search, information retrieval |
| `GLM-4.5-Air` | `0727-106B-API` | Lightweight model | Fast response, efficient inference |
| `GLM-4.6` | `GLM-4-6-API-V1` | Latest version | Improved capabilities over 4.5 |
| `GLM-4.6-Thinking` | `GLM-4-6-API-V1` | Latest reasoning | Enhanced reasoning with v4.6 |
| `GLM-4.6-Search` | `GLM-4-6-API-V1` | Latest search | Improved search with v4.6 |

### Features & Capabilities

#### 1. **Streaming Support** ‚úÖ
- Real-time Server-Sent Events (SSE) streaming
- Chunk-by-chunk response delivery
- Supports both streaming and non-streaming modes

#### 2. **Tool Calling / Function Calling** ‚úÖ
- Native OpenAI-style function calling support
- MCP (Model Context Protocol) server integration
- Supported MCP servers:
  - `vibe-coding` - Code generation assistance
  - `ppt-maker` - PowerPoint generation
  - `image-search` - Image discovery
  - `deep-research` - In-depth research
  - `deep-web-search` - Web search (for Search models)
  - `tool_selector` - Dynamic tool selection
  - `advanced-search` - Enhanced search capabilities

#### 3. **Thinking Mode** üß†
- Enables reasoning process visibility
- Returns both reasoning content and final answer
- Available in `-Thinking` model variants
- Reasoning content returned in `reasoning_content` field

#### 4. **Web Search** üîç
- Integrated web search for Search model variants
- Automatic search result integration
- Configurable search parameters
- Real-time information retrieval

#### 5. **Multi-turn Conversations** üí¨
- Full conversation history support
- System, user, and assistant roles
- Context preservation across turns

### Authentication Mechanisms

#### Token Pool System
```python
# Multiple token rotation with automatic failover
AUTH_TOKENS_FILE=tokens.txt  # One token per line or comma-separated
TOKEN_FAILURE_THRESHOLD=3    # Failures before marking unavailable
TOKEN_RECOVERY_TIMEOUT=1800  # Recovery time in seconds
```

#### Anonymous Mode
```python
ANONYMOUS_MODE=true  # Automatically obtains visitor tokens
```

**Flow**:
1. Try token pool first (if configured)
2. Fall back to configured AUTH_TOKEN
3. Use anonymous visitor tokens (if enabled)
4. Automatic token rotation on failure

### Request Transformation

**Input**: OpenAI format
```json
{
  "model": "GLM-4.5-Thinking",
  "messages": [
    {"role": "user", "content": "Explain quantum computing"}
  ],
  "temperature": 0.7,
  "stream": true
}
```

**Output**: Z.AI format
```json
{
  "stream": true,
  "model": "0727-360B-API",
  "messages": [...],
  "features": {
    "web_search": false,
    "enable_thinking": true,
    "image_generation": false
  },
  "params": {}
}
```

### Error Handling

- **Token Failures**: Automatic rotation to next available token
- **Rate Limiting**: Exponential backoff with retry (max 6 attempts)
- **Connection Errors**: Graceful degradation to anonymous mode
- **Invalid Responses**: Comprehensive error logging and user-friendly messages

---

## üß© K2Think Provider

**File**: `app/providers/k2think_provider.py`  
**Base URL**: `https://www.k2think.ai`  
**Status**: ‚úÖ Fully Implemented

### Supported Models

| Model Name | Description | Key Features |
|------------|-------------|--------------|
| `MBZUAI-IFM/K2-Think` | Advanced reasoning model | Fast inference, high-quality reasoning |

### Features & Capabilities

#### 1. **Streaming Support** ‚úÖ
- Real-time SSE streaming
- Efficient token-by-token delivery

#### 2. **Reasoning Mode** üß†
- Deep reasoning capabilities
- Step-by-step problem solving
- Transparent thought process

#### 3. **Guest Mode** üîì
- No authentication required
- Automatic session management via cookies
- Public access model

### Authentication Mechanisms

**Cookie-Based Session**:
- Automatic cookie extraction from responses
- Session persistence across requests
- No manual token management required

```python
# No explicit auth config needed
# Provider handles session automatically
```

### Request Transformation

**Input**: OpenAI format
```json
{
  "model": "MBZUAI-IFM/K2-Think",
  "messages": [{"role": "user", "content": "Hello"}],
  "stream": true
}
```

**Output**: K2Think format
```json
{
  "model": "MBZUAI-IFM/K2-Think",
  "stream": true,
  "messages": [...],
  "temperature": 0.7
}
```

### Error Handling

- **Session Expiry**: Automatic session renewal
- **Rate Limiting**: Graceful backoff
- **Connection Errors**: Detailed error reporting

---

## üê± LongCat Provider

**File**: `app/providers/longcat_provider.py`  
**Base URL**: `https://longcat.chat`  
**Status**: ‚úÖ Fully Implemented

### Supported Models

| Model Name | Description | Key Features |
|------------|-------------|--------------|
| `LongCat-Flash` | Fast response model | High-speed inference, real-time conversations |
| `LongCat` | Standard model | Balanced performance, general purpose |
| `LongCat-Search` | Search-enabled model | Web search integration, information retrieval |

### Features & Capabilities

#### 1. **Streaming Support** ‚úÖ
- Real-time SSE streaming
- Chunk-based response delivery

#### 2. **Web Search** üîç
- Integrated search (Search variant only)
- Real-time information retrieval
- Automatic result formatting

#### 3. **Multi-turn Conversations** üí¨
- Full conversation history
- Context awareness

### Authentication Mechanisms

**Passport Token System**:

Option 1: Single token via environment variable
```bash
LONGCAT_PASSPORT_TOKEN=your_passport_token_here
```

Option 2: Multiple tokens via file
```bash
LONGCAT_TOKENS_FILE=longcat_tokens.txt
```

**Token File Format**:
```
token1
token2
token3
```

### Request Transformation

**Input**: OpenAI format
```json
{
  "model": "LongCat-Search",
  "messages": [{"role": "user", "content": "What's the weather?"}],
  "stream": true
}
```

**Output**: LongCat format
```json
{
  "model": "LongCat-Search",
  "stream": true,
  "messages": [...],
  "passport": "passport_token_here"
}
```

### Error Handling

- **Token Validation**: Checks for valid passport tokens
- **Token Rotation**: Random selection from token file
- **Connection Errors**: Comprehensive error logging

---

## ‚ùì Qwen Provider

**Status**: ‚ùå Not Yet Implemented

### Planned Models

Based on documentation references:
- `qwen-max` - Most capable model
- `qwen-max-thinking` - Reasoning mode
- `qwen-max-search` - Web search capability
- `qwen-max-image` - Image generation
- `qwen-plus` - Balanced performance
- `qwen-turbo` - Fast responses
- `qwen-long` - Extended context

### Implementation Status

This provider is referenced in the README but not yet implemented in code. To implement:

1. Create `app/providers/qwen_provider.py`
2. Implement authentication mechanism
3. Add model mapping and transformation logic
4. Register with provider factory

---

## üîÑ Provider Factory & Model Routing

**File**: `app/providers/provider_factory.py`

### How Routing Works

1. **Model-to-Provider Mapping**: Each model name is registered to a specific provider
2. **Automatic Selection**: Request model name determines which provider handles the request
3. **Fallback Logic**: If provider unavailable, error is returned to client

### Model Registry

```python
from app.providers.base import provider_registry

# List all available models
models = provider_registry.list_models()

# Get provider for specific model
provider = provider_registry.get_provider("GLM-4.5")

# List all providers
providers = provider_registry.list_providers()
```

---

## üß™ Testing Provider Capabilities

### Test Each Model

```bash
# Test Z.AI models
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "GLM-4.5-Thinking",
    "messages": [{"role": "user", "content": "Explain AI"}],
    "stream": false
  }'

# Test K2Think
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "MBZUAI-IFM/K2-Think",
    "messages": [{"role": "user", "content": "Hello"}]
  }'

# Test LongCat
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "LongCat-Search",
    "messages": [{"role": "user", "content": "Latest news?"}]
  }'
```

### Test Tool Calling (Z.AI only)

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "GLM-4.5",
    "messages": [{"role": "user", "content": "Search for AI news"}],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "web_search",
          "description": "Search the web",
          "parameters": {
            "type": "object",
            "properties": {
              "query": {"type": "string"}
            }
          }
        }
      }
    ]
  }'
```

---

## üìà Performance Comparison

| Provider | Avg Response Time | Max Context | Cost (est.) | Availability |
|----------|------------------|-------------|-------------|--------------|
| Z.AI | ~2-5s | 128K tokens | Free/Paid tiers | High |
| K2Think | ~1-3s | 32K tokens | Free (guest) | Medium |
| LongCat | ~1-4s | 64K tokens | Token-based | High |

---

## üîß Provider Configuration Best Practices

### For Production Deployments

1. **Z.AI**: Use token pool with multiple tokens for load balancing
2. **K2Think**: Monitor guest mode rate limits
3. **LongCat**: Maintain multiple passport tokens for redundancy

### Security Considerations

- Store tokens in environment variables, not in code
- Use `.env` files (not committed to git)
- Rotate tokens regularly
- Monitor for token leakage in logs

### Monitoring & Logging

All providers log:
- Request/response cycles
- Token usage and failures
- Error conditions
- Performance metrics

Check logs in `logs/` directory for detailed diagnostics.

---

## üìö Additional Resources

- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [Z.AI Official Site](https://chat.z.ai)
- [K2Think Platform](https://www.k2think.ai)
- [LongCat Chat](https://longcat.chat)

---

**Last Updated**: 2025-01-06  
**Document Version**: 1.0

